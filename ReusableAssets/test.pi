import boto3
from pyspark.sql import SparkSession

s3 = boto3.client('s3')
bucket_name = 'your-bucket-name'
format_file_key = 'path/to/format-file.fmt'
input_file_key = 'path/to/input-file.txt'
output_file_key = 'path/to/output-file.avro'

# Read format file from S3
format_file = s3.get_object(Bucket=bucket_name, Key=format_file_key)['Body'].read().decode('utf-8')

# Parse format file to get column lengths and names
columns = {}
for line in format_file.splitlines():
    column_name, column_length = line.split()
    columns[column_name] = int(column_length)

# Create SparkSession
spark = SparkSession.builder.appName('FixedWidthToAvro').getOrCreate()

# Read fixed-width file as RDD
input_rdd = spark.sparkContext.textFile(f's3://{bucket_name}/{input_file_key}')

# Split lines into fixed-width columns
split_rdd = input_rdd.map(lambda line: [line[start:start+length].strip() for start, length in columns.items()])

# Convert RDD to DataFrame
df = split_rdd.toDF(columns.keys())

# Write DataFrame to Avro
df.write.format('com.databricks.spark.avro').save(f's3://{bucket_name}/{output_file_key}')

#wget https://repo1.maven.org/maven2/com/databricks/spark-avro_2.11/4.0.0/spark-avro_2.11-4.0.0.jar
