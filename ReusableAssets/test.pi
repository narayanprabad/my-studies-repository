from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

# Define the format configuration
format_config = [
    {"name": "id", "start": 1, "end": 10},
    {"name": "name", "start": 11, "end": 30},
    {"name": "age", "start": 31, "end": 32},
    {"name": "gender", "start": 33, "end": 33},
]

# Define the Avro schema
avro_schema = StructType([
    StructField("id", StringType(), True),
    StructField("name", StringType(), True),
    StructField("age", IntegerType(), True),
    StructField("gender", StringType(), True)
])

# Define the S3 input and output paths
input_path = "s3://my-bucket/input-file.txt"
output_path = "s3://my-bucket/output-file.avro"

# Define the SparkSession
spark = SparkSession.builder.appName("FixedWidthParser").getOrCreate()

# Read the input file as a text file
input_rdd = spark.sparkContext.textFile(input_path)

# Define a UDF to parse the fixed-width file
@udf(returnType=avro_schema)
def parse_fixed_width_line(line):
    # Parse the line based on the format configuration
    fields = {}
    for field in format_config:
        start = field["start"] - 1
        end = field["end"]
        fields[field["name"]] = line[start:end].strip()

    # Perform some enrichment on the data
    fields["age"] = int(fields["age"]) * 2
    fields["gender"] = "M" if fields["gender"] == "M" else "F"

    # Return the parsed and enriched data as a Row object
    return Row(**fields)

# Convert the text file to a DataFrame and parse each line using the UDF
df = input_rdd.map(parse_fixed_width_line).toDF()

# Write the result to S3 in Avro format
df.write.format("com.databricks.spark.avro").save(output_path)

# Stop the SparkSession
spark.stop()

# wget https://repo1.maven.org/maven2/com/databricks/spark-avro_2.11/4.0.0/spark-avro_2.11-4.0.0.jar

# pyspark --packages com.databricks:spark-avro_2.11:4.0.0 --driver-class-path spark-avro_2.11-4.0.0.jar
